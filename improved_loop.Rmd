---
title: "improved_loop"
author: "Juliet"
date: "11/25/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(here)
library(tidyverse)
library(janitor)
library(lubridate)
library(broom)
library(forecast)
library(purrr)
```

```{r}
data = read_csv(here('data', 'fishing-vessels-v2.csv'))
```
Initial data wrangling

```{r}
# clean the data, selecting only relevant column of fishing hours and taking the means by year for each country

effort_trends <- data %>% 
  select(flag_gfw, 
         fishing_hours_2012,
         fishing_hours_2013,
         fishing_hours_2014,
         fishing_hours_2015,
         fishing_hours_2016,
         fishing_hours_2017,
         fishing_hours_2018,
         fishing_hours_2019,
         fishing_hours_2020) %>% 
  group_by(flag_gfw) %>% 
  summarize("2012" = mean(fishing_hours_2012, na.rm = TRUE),
            "2013" = mean(fishing_hours_2013, na.rm = TRUE),
            "2014" = mean(fishing_hours_2014, na.rm = TRUE),
            "2015" = mean(fishing_hours_2015, na.rm = TRUE),
            "2016" = mean(fishing_hours_2016, na.rm = TRUE),
            "2017" = mean(fishing_hours_2017, na.rm = TRUE),
            "2018" = mean(fishing_hours_2012, na.rm = TRUE),
            "2019" = mean(fishing_hours_2012, na.rm = TRUE),
            "2020" = mean(fishing_hours_2020, na.rm = TRUE))

```

Determine which year is the best start year for the linear regression by finding the year with the lest NA values (because in our for loop, we will multiply the slope coefficient of each country's model by the same numerical value for each country when predicting their 2020 fishing effort, so we need all countries to have the same start year of data)

```{r}
# which column has the least NA values? We will use this to determine which year is the best start year for our linear regression
sum(is.na(effort_trends$"2012"))
sum(is.na(effort_trends$"2013"))
sum(is.na(effort_trends$"2014"))
sum(is.na(effort_trends$"2015"))
sum(is.na(effort_trends$"2016"))
sum(is.na(effort_trends$"2017"))
# 2017 has the least NA values, so we will remove all countries that dont have data for 2017
```

Turn the data into Tidy format so we can run linear regressions over the years, and only include fishing effort for 2017-2019 because we want to regress over those years only

```{r}
# change it to tidy format, and remove all NA values, and take out the year 2020 because we want to compare what we would EXPECT in 2020 based on what we saw in 2012-2019

effort_trends_tidy_no_na = effort_trends %>%
  select(flag_gfw, "2017":"2019") %>% 
  pivot_longer(cols = ("2017":"2019"),
               names_to = "year",
               values_to = "mean_effort") %>% 
  filter(!is.na(mean_effort),
         !is.na(flag_gfw))
```

Remove all countries that only occur once, because we need at least 2 years of fishing effort data per country to run a linear model for each country.

```{r}
# remove those countries from the dataframe
countries_clean <- effort_trends_tidy_no_na %>% 
  group_by(flag_gfw) %>%
  filter(n()>1) %>% 
  mutate(year = as.Date(year, format = "%Y"))
# 83 coutnries remain in this dataset
```

Run a simple linear regression for just one country, as a test. Plot it to visualize the fishing effort trend.

```{r}
# ARG = Argentina 
arg_data <- countries_clean %>% 
  filter(flag_gfw == "ARG") %>% 
  mutate(year = as.Date(year, format = "%Y"))
# make sure to turn the years into dates so we run a time series, otherwise the years will be considered categories

# run a linear model on a single country
arg_model = lm(mean_effort ~ year, data = arg_data)
summary(arg_model)

ggplot(data = arg_data, aes(x = year, y = mean_effort, color = year)) +
   geom_point() +
   geom_line(data = augment(arg_model), aes(y = .fitted, color = year)) + 
   labs(x = "Year",
        y = "Mean Fishing Hours",
        title = "Argentina's Fishing Effort from 2017-2019")
```

Look at another country with a negative intercept (the Caribbean Netherlands) to see what that data looks like.

```{r}
can_data <- countries_clean %>% 
  filter(flag_gfw == "CAN") %>% 
  mutate(year = as.Date(year, format = "%Y"))
# make sure to turn the years into dates so we run a time series, otherwise the years will be considered categories

# run a linear model on a single country
can_model = lm(mean_effort ~ year, data = can_data)
summary(can_model)

# when you plot this one, do not include the geom line aspect because it is not fitting the data correctly at all
ggplot(data = can_data, aes(x = year, y = mean_effort, color = year)) +
   geom_point() +
   geom_line(data = augment(can_model), aes(y = .fitted, color = year)) + 
   labs(x = "Year",
        y = "Mean Fishing Hours",
        title = "Canada's Fishing Effort from 2017-2019")
```

Use the `sapply()` function to iterate through all the countries and run a linear model. Then, concatenate those linear model summaries using `lapply()`.

```{r}
# iterate through the models, running lm() on each country
# models <- sapply(unique(as.character(countries_clean$flag_gfw)),
#                  function(country)lm(mean_effort~year, countries_clean, subset = (flag_gfw == country)),
#                  simplify = FALSE, USE.NAMES = TRUE)
# 
# # summarize all the models
# countries_lm_summaries <- lapply(models, summary)
# 
# # call the first model
# arg_summary <- models[["ARG"]]
# # try to extract the coefficients from the model
#arg_summary$coefficients[,"Estimate"][2]
# this fails to return the slope coefficient, it returns NULL
#models[["ARG"]]$coefficients[,"Estimate"][2]
# this fails to return the slope coefficient, it returns NULL

#try calling the coefficients from the output of the summary of all the models
# call the first summary
#countries_lm_summaries[1]
#arg_summary <- countries_lm_summaries[1]
#arg_summary$coefficients[,"Estimate"][2]
# this also returns NULL
#class(arg_summary)

# try using unnest()
#coeff <- sapply(countries_clean$models, function(x) return(coefficients(x)))

#bind_cols(countries_clean, data.frame(t(coeff))) %>% 
#     rename_at(6:8, ~ c("intc",  "coef1",  "coef2")) %>% 
#          distinct(flag_gfw, .keep_all = TRUE)

# try to use the do() function

countries_clean %>% 
  group_by(flag_gfw) %>% 
  do(data.frame(., as.list(coef(lm(mean_effort~year, .))))) #%>%
  #rename_at(5:7, ~c("intc", "coef1", "coef2"))

# try to adjust the code that worked earlier to be like this code
models <- sapply(unique(as.character(countries_clean$flag_gfw)),
                 function(country)as.numeric(coef(lm(mean_effort~year, countries_clean, subset = (flag_gfw == country)))),
                 simplify = FALSE, USE.NAMES = TRUE)
models[[4]]
# retunrs just that country's coefficients
#models$ARG[1]
#countries_lm_summaries <- lapply(models, summary)
#countries_lm_summaries
```

```{r}
# this for loop iterates through the lm summaries of each country, and should put the predicted fishing effort for 2020 into the empty dataframe I create called "prediction_data"

# prediction_data = NULL;
# for (i in 1:length(countries_lm_summaries)) {
#   one_country_summary <- countries_lm_summaries[i]
#   print(one_country_summary)
#   intercept <- one_country_summary$coefficients[,"Estimate"][1]
#   print(intercept)
#   slope <- one_country_summary$coefficients[,"Estimate"][2]
#   print(slope)
#   predicted_effort_2020 <- intercept + slope*3
#   prediction_data <- rbind(prediction_data, predicted_effort_2020)
#   print(paste0("In 2020, the predicted fishing hours is ", predicted_effort_2020))
# }
```

```{r}
prediction_data = NULL;
for (i in 1:length(models)) {
  predicted_effort_2020 <- models[[i]][1] + models[[i]][2]*3
  prediction_data <- rbind(prediction_data, predicted_effort_2020)
  print(paste0("In 2020, the predicted fishing hours is ", predicted_effort_2020))
}
```
Combine the predicted 2020 fishing effort data with the actual 2020 fishing effort data to compare by country.

```{r}
# figure out which countries were used in the for loop so we can get the actual 2020 effort data for those countries only
countries_clean_unique <- countries_clean %>% 
  group_by(flag_gfw) %>%
  slice_head(n = 1)

# set these countries as a vector so we can subset the effort_trends data to only include those countries
countries_to_compare <- unique(countries_clean_unique$flag_gfw)
countries_to_compare
  
# ensure that there are 83 rows in both datasets
nrow(countries_clean_unique)
nrow(prediction_data)

# set the effort trends data to only include those countries
comparison_2020 <- effort_trends %>% 
  select(flag_gfw, "2020") %>%
  rename(actual_2020 = "2020") %>% 
  filter(str_detect(flag_gfw, paste(countries_to_compare, collapse="|"))) %>% 
  cbind(prediction_data) %>% 
  rename(prediction_2020 = prediction_data) %>% 
  filter(actual_2020 != "NaN")
# I made sure to remove the NAN values from the countries that did not have actual data for 2020 AFTER I USED cbind() because I wanted to bind the actual 2020 data to the corresponding rows with the predicted data first or else the alignment would yield incorrect data
```

Do a hypothesis test on the difference between the predicted mean and the average mean.

**Null Hypothesis:** There is no difference between the predicted country-specific predicted fishing effort in 2020 and the actual country-specific fishing effort in 2020.
**Alternative Hypothesis:** There is a difference between the predicted country-specific predicted fishing effort in 2020 and the actual country-specific fishing effort in 2020. Because of the pandemic in 2020, I predict that fishing effort decreased, meaning that the actual country-specific fishing effort is less than the predicted country-specific fishing effort.
```{r}
# running a ttest actual ~ predict fails with the comparison_2020 data AND it also fails running with comparison_2020 data filtered to only one country...because there are more than 2 levels (?) so maybe we need to run difference_act_pred ~ country with ANOVA? seems out of the scope of this class

#ttest_practice_data <- comparison_2020 %>% 
#  filter(flag_gfw == "ARG")

#ttest = t.test(actual_2020 ~ prediction_2020, data = ttest_practice_data, conf.level = 0.95)
#ttest

#p_val <- ttest$p.value
#p_val

# consider instead of a ttest, maybe we should run a regression thing to see how much error there was between the predicted and the actual
```

## Consider removing the negative predicted values from the fishing data...they are impossible intercepts
## Consider changing the explanation for filtering for countries from 2017-2020 to train the lm model, the more reasonable reasoning is that 2017-2019 is 3 yrs of data that leads up to 2020, most predictive due to it being recent and it gives more countries than doing 2016-2019 since we have to remove all NA values












